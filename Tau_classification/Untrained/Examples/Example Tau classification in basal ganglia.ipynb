{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example script: training tau classifier for basal ganglia"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in relevant files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\n",
    "                '/Users/mokur/OneDrive - University of Cambridge/Attachments/Jan2023/Tau_pipeline/Tau_classification/')\n",
    "\n",
    "from base import *\n",
    "from constants import *\n",
    "from tau_classification import * "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Screening classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tau        10185\n",
      "Non_tau     9963\n",
      "Name: Class, dtype: int64\n",
      "(20148, 54)\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/mokur/OneDrive - University of Cambridge/Attachments/Jan2023/Tau_pipeline/Tau_classification/Training_data/screening_classifier/\"\n",
    "filename = \"training.txt\"\n",
    "\n",
    "# Create tau database object \n",
    "s_data = TauDataBase(path = path,\n",
    "                     filename = filename) \n",
    "\n",
    "# Prepping data to train screening classifier\n",
    "s_data.classifier1_prep()\n",
    "\n",
    "# Check data\n",
    "print(s_data.c1_data['Class'].value_counts())\n",
    "print(s_data.c1_X_train.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tau classifier for Striatum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Others    3116\n",
      "CB         335\n",
      "TA         200\n",
      "NFT         48\n",
      "Name: Class, dtype: int64\n",
      "(3699, 54)\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/mokur/OneDrive - University of Cambridge/Attachments/Jan2023/Tau_pipeline/Tau_classification/Untrained/Training_data/STR/\"\n",
    "filename = \"training.txt\"\n",
    "\n",
    "# Create tau database object \n",
    "str_data = TauDataBase(path = path,\n",
    "                       filename = filename) \n",
    "\n",
    "# Prepping data to train tau classifier for cortical regions\n",
    "str_data.classifier2_prep()\n",
    "\n",
    "# Check data\n",
    "print(str_data.c2_data['Class'].value_counts())\n",
    "print(str_data.c2_X_train.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tau classifier for Subthalamic nucleus & globus pallidus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Others    12988\n",
      "CB          601\n",
      "NFT          97\n",
      "Name: Class, dtype: int64\n",
      "(13686, 54)\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/mokur/OneDrive - University of Cambridge/Attachments/Jan2023/Tau_pipeline/Tau_classification/Untrained/Training_data/STN&GP/\"\n",
    "filename = \"training.txt\"\n",
    "\n",
    "# Create tau database object \n",
    "stn_gp_data = TauDataBase(path = path,\n",
    "                          filename = filename) \n",
    "\n",
    "# Prepping data to train tau classifier for cortical regions\n",
    "stn_gp_data.classifier2_prep()\n",
    "\n",
    "# Check data\n",
    "print(stn_gp_data.c2_data['Class'].value_counts())\n",
    "print(stn_gp_data.c2_X_train.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising & training the classifiers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Screening classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('normalizer', MinMaxScaler()),\n",
       "                ('selector',\n",
       "                 RFE(estimator=RandomForestClassifier(random_state=42),\n",
       "                     n_features_to_select=44)),\n",
       "                ('clf',\n",
       "                 BalancedRandomForestClassifier(class_weight='balanced',\n",
       "                                                max_features=1,\n",
       "                                                max_samples=0.75,\n",
       "                                                min_samples_leaf=2,\n",
       "                                                n_estimators=300,\n",
       "                                                random_state=42,\n",
       "                                                sampling_strategy='not '\n",
       "                                                                  'majority'))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screening_model = ScreeningClassifier(hyperparameters=screening_classifier_hyperparams)\n",
    "screening_model.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "screening_model.train(X=s_data.c1_X_train,\n",
    "                      Y=s_data.c1_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ROI: 0.25 µm per pixel: DAB: Max</td>\n",
       "      <td>0.053625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ROI: 0.25 µm per pixel: DAB: Haralick Sum aver...</td>\n",
       "      <td>0.048330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ROI: 0.25 µm per pixel: Green: Mean</td>\n",
       "      <td>0.041939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ROI: 0.25 µm per pixel: DAB: Mean</td>\n",
       "      <td>0.041595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ROI: 0.25 µm per pixel: Red: Mean</td>\n",
       "      <td>0.040228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             features  importance\n",
       "20                   ROI: 0.25 µm per pixel: DAB: Max    0.053625\n",
       "16  ROI: 0.25 µm per pixel: DAB: Haralick Sum aver...    0.048330\n",
       "26                ROI: 0.25 µm per pixel: Green: Mean    0.041939\n",
       "21                  ROI: 0.25 µm per pixel: DAB: Mean    0.041595\n",
       "35                  ROI: 0.25 µm per pixel: Red: Mean    0.040228"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screening_model.f_importance.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tau classifier for striatum regions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('normalizer', MinMaxScaler()),\n",
       "                ('selector',\n",
       "                 RFE(estimator=RandomForestClassifier(random_state=42),\n",
       "                     n_features_to_select=34)),\n",
       "                ('clf',\n",
       "                 BalancedRandomForestClassifier(class_weight='balanced',\n",
       "                                                max_depth=15, max_features=0.6,\n",
       "                                                max_samples=0.75,\n",
       "                                                min_samples_leaf=2,\n",
       "                                                n_estimators=500,\n",
       "                                                random_state=42,\n",
       "                                                sampling_strategy='not '\n",
       "                                                                  'majority'))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_model = TauClassifier(hyperparameters=str_classifier_hyperparams)\n",
    "str_model.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "str_model.train(X=str_data.c2_X_train,\n",
    "                Y=str_data.c2_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (0.19564858129927834,\n",
       "  0.8870751195750726,\n",
       "  0.873128875762111,\n",
       "  0.9072192513368984),\n",
       " 1: (0.8264912975852534, 0.9777777777777779, 1.0, 0.96),\n",
       " 2: (0.7613745234867089,\n",
       "  0.991988553025763,\n",
       "  0.9920257276937077,\n",
       "  0.9919779041965537),\n",
       " 3: (0.45007735002598476,\n",
       "  0.9372955624546812,\n",
       "  0.914380764163373,\n",
       "  0.9650000000000001)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_model.best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Area µm^2</td>\n",
       "      <td>0.214311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Min diameter µm</td>\n",
       "      <td>0.181223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ROI: 0.25 µm per pixel: DAB: Mean</td>\n",
       "      <td>0.168029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ROI: 0.25 µm per pixel: DAB: Median</td>\n",
       "      <td>0.113468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ROI: 0.25 µm per pixel: DAB: Haralick Sum aver...</td>\n",
       "      <td>0.079985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             features  importance\n",
       "0                                           Area µm^2    0.214311\n",
       "4                                     Min diameter µm    0.181223\n",
       "19                  ROI: 0.25 µm per pixel: DAB: Mean    0.168029\n",
       "20                ROI: 0.25 µm per pixel: DAB: Median    0.113468\n",
       "16  ROI: 0.25 µm per pixel: DAB: Haralick Sum aver...    0.079985"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_model.f_importance.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tau classifier for subthalamic nucleus, globus pallidus regions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('normalizer', MinMaxScaler()),\n",
       "                ('selector',\n",
       "                 RFE(estimator=RandomForestClassifier(random_state=42),\n",
       "                     n_features_to_select=34)),\n",
       "                ('clf',\n",
       "                 BalancedRandomForestClassifier(class_weight='balanced',\n",
       "                                                max_depth=15, max_features=0.6,\n",
       "                                                max_samples=0.75,\n",
       "                                                min_samples_leaf=2,\n",
       "                                                n_estimators=500,\n",
       "                                                random_state=42,\n",
       "                                                sampling_strategy='not '\n",
       "                                                                  'majority'))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stn_gp_model = TauClassifierNoTA(hyperparameters=stn_gp_classifier_hyperparams)\n",
    "stn_gp_model.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "stn_gp_model.train(X=stn_gp_data.c2_X_train,\n",
    "                   Y=stn_gp_data.c2_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (0.18845399342448405,\n",
       "  0.8946962279425845,\n",
       "  0.8965841716432827,\n",
       "  0.9002459016393443),\n",
       " 1: (0.7187865615771717,\n",
       "  0.9529564106344293,\n",
       "  0.9518181818181819,\n",
       "  0.9588888888888889),\n",
       " 2: (0.774700336310272,\n",
       "  0.9956218336556081,\n",
       "  0.9939498939499222,\n",
       "  0.9973053231655025)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stn_gp_model.best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Area µm^2</td>\n",
       "      <td>0.510403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Min diameter µm</td>\n",
       "      <td>0.193240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ROI: 0.25 µm per pixel: DAB: Mean</td>\n",
       "      <td>0.074044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Max diameter µm</td>\n",
       "      <td>0.031333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Solidity</td>\n",
       "      <td>0.028362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             features  importance\n",
       "0                           Area µm^2    0.510403\n",
       "4                     Min diameter µm    0.193240\n",
       "21  ROI: 0.25 µm per pixel: DAB: Mean    0.074044\n",
       "3                     Max diameter µm    0.031333\n",
       "33                           Solidity    0.028362"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stn_gp_model.f_importance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['str_classifier.sav']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(stn_gp_model, 'stn_gp_classifier.sav')\n",
    "joblib.dump(str_model, 'str_classifier.sav')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together: tau classification pipeline for novel *cortical* slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_path = '/Users/mokur/OneDrive - University of Cambridge/Attachments/Jan2023/Detections/BG/'\n",
    "novel_filename = 'all_files.txt'\n",
    "prediction_path = \"/Users/mokur/OneDrive - University of Cambridge/Attachments/Jan2023/Predictions_new/BG/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading in files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(novel_path + novel_filename) as f:\n",
    "    mylist = f.read().splitlines()\n",
    "\n",
    "print(\"Read in: \", len(mylist), \"files\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tau classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = {'GP':'GP',\n",
    "        'STN':'STN',\n",
    "        'STR':'STR',\n",
    "        'GP_reseg':'GP',\n",
    "        'STN_reseg':'STN',\n",
    "        'STR_reseg':'STR'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE 747370.svs Detections.txt Number:  1 / 1\n",
      "---------------STEP1: Read in data file -------------------\n",
      "{'GP', 'STN', 'STR'}\n",
      "Read in data file: 747370.svs Detections.txt\n",
      "Data shape is: (46246, 61)\n",
      "---------------STEP2: Separating Non-tau from Tau -------------------\n",
      "Non_tau    41737\n",
      "Tau         4509\n",
      "Name: Class, dtype: int64\n",
      "---------------STEP3: Tau hallmark classification -------------------\n",
      "Tau on STR:  775\n",
      "Tau on STN & GP:  3734\n",
      "STR predicted slide shape:  Others       744\n",
      "CB            17\n",
      "Ambiguous      9\n",
      "NFT            5\n",
      "Name: Class, dtype: int64\n",
      "STN_GP predicted slide shape:  Others       3417\n",
      "CB            239\n",
      "NFT            41\n",
      "Ambiguous      37\n",
      "Name: Class, dtype: int64\n",
      "---------------STEP4: Data extraction & export -------------------\n",
      "No loss of cells?  True\n",
      "Exported prediction of :  747370.svs Detections.txt\n",
      "---------------------------------------------------\n",
      "Well done, no error!\n"
     ]
    }
   ],
   "source": [
    "n_total = len(mylist)\n",
    "faulty_file = []\n",
    "for i in range(0, n_total):\n",
    "\n",
    "    # Read in novel slide\n",
    "    print(\"FILE\", mylist[i], \"Number: \", i + 1, \"/\", n_total)\n",
    "    print(\"---------------STEP1: Read in data file -------------------\")\n",
    "    dat_file = mylist[i]\n",
    "\n",
    "    dat_ = pd.read_csv(novel_path + dat_file, sep=\"\\t\")\n",
    "\n",
    "    # Fixing order of the columns\n",
    "    ordered = dat_[extracted_features]\n",
    "\n",
    "    # Changing column names\n",
    "    # since these names tend to be inconsistent causing problems\n",
    "    ordered.columns.values[5] = \"Centroid_X\"\n",
    "    ordered.columns.values[6] = \"Centroid_Y\"\n",
    "\n",
    "    dat_ordered = ordered[ordered[\"Class\"] == \"Unlabelled\"]  # only unlabelled cells\n",
    "    dat_parent = dat_ordered['Parent']\n",
    "    dat = dat_ordered.copy()\n",
    "    dat.loc[:, 'Parent'] = [code[i] for i in dat_parent]\n",
    "    print(set(dat['Parent']))\n",
    "    print(\"Read in data file:\", dat_file)\n",
    "    print(\"Data shape is:\", dat.shape)\n",
    "\n",
    "    # Classifier 1: separating Non-tau from Tau\n",
    "    print(\n",
    "        \"---------------STEP2: Separating Non-tau from Tau -------------------\"\n",
    "    )\n",
    "    # 1) Remove NA cells\n",
    "    dat = dat.dropna()\n",
    "\n",
    "    predicted1_slide = dat.copy()\n",
    "\n",
    "    # 2) Dropping extra info features\n",
    "    X_unlabelled = dat.drop(\n",
    "        columns=[\"Image\",\n",
    "                 \"Name\",\n",
    "                 \"Class\",\n",
    "                 \"Parent\",\n",
    "                 \"ROI\",\n",
    "                 \"Centroid_X\",\n",
    "                 \"Centroid_Y\"]\n",
    "    )\n",
    "    # 3) Predictions\n",
    "    screening_model.predict(X_unlabelled)\n",
    "    predicted1_slide[\"Class\"] = screening_model.prediction\n",
    "    print(predicted1_slide[\"Class\"].value_counts())\n",
    "\n",
    "    # Classifier 2: tau hallmark classification\n",
    "    print(\n",
    "        \"---------------STEP3: Tau hallmark classification -------------------\"\n",
    "    )\n",
    "    # Select out 'tau' portion only (ignoring non-tau & ambiguous cells )\n",
    "    tau_portion = predicted1_slide[predicted1_slide[\"Class\"] == \"Tau\"]\n",
    "    if tau_portion.shape[0] == 0:\n",
    "        print(\"There is no tau on this slide!\")\n",
    "        faulty_file.append(dat[\"Image\"][0] + \" No tau on the slide\")\n",
    "        continue\n",
    "        \n",
    "    # Split BG into STR and GP & STN\n",
    "    \n",
    "    tau_portion_STR = tau_portion[tau_portion['Parent']=='STR']\n",
    "    print('Tau on STR: ', tau_portion_STR.shape[0])\n",
    "    \n",
    "    if tau_portion_STR.shape[0] == 0:\n",
    "        print(\"THERE IS NO TAU ON THIS SLIDE\")\n",
    "        faulty_file.append(dat[\"Image\"][0] + \" no tau on the slide\")\n",
    "        continue\n",
    "    \n",
    "    tau_portion_STN_GP = tau_portion[(tau_portion['Parent']=='STN')|(tau_portion['Parent']=='GP')]\n",
    "    print('Tau on STN & GP: ', tau_portion_STN_GP.shape[0])\n",
    "    \n",
    "    if tau_portion_STN_GP.shape[0] == 0:\n",
    "        print(\"THERE IS NO TAU ON THIS SLIDE\")\n",
    "        faulty_file.append(dat[\"Image\"][0] + \" no tau on the slide\")\n",
    "        continue\n",
    "    \n",
    "    # STR prediction\n",
    "    tau_portion_STR_X = tau_portion_STR.drop(\n",
    "        columns=[\"Image\",\n",
    "                 \"Name\",\n",
    "                 \"Class\",\n",
    "                 \"Parent\",\n",
    "                 \"ROI\",\n",
    "                 \"Centroid_X\",\n",
    "                 \"Centroid_Y\"]\n",
    "    )\n",
    "    predicted2_slide_STR = tau_portion_STR.copy()\n",
    "\n",
    "\n",
    "    # 1) Get class probability predictions for 'test' data\n",
    "    str_model.predict(tau_portion_STR_X)\n",
    "\n",
    "    predicted2_slide_STR[\"Class\"] = str_model.prediction\n",
    "    print('STR predicted slide shape: ',predicted2_slide_STR[\"Class\"].value_counts())\n",
    "    \n",
    "    # GP&STN \n",
    "    tau_portion_STN_GP_X = tau_portion_STN_GP.drop(\n",
    "        columns=[\"Image\",\n",
    "                 \"Name\",\n",
    "                 \"Class\",\n",
    "                 \"Parent\",\n",
    "                 \"ROI\",\n",
    "                 \"Centroid_X\",\n",
    "                 \"Centroid_Y\"]\n",
    "    )\n",
    "    predicted2_slide_STN_GP = tau_portion_STN_GP.copy()\n",
    "\n",
    "\n",
    "    # 1) Get class probability predictions for 'test' data\n",
    "    stn_gp_model.predict(tau_portion_STN_GP_X)\n",
    "\n",
    "    predicted2_slide_STN_GP[\"Class\"] = stn_gp_model.prediction\n",
    "    print('STN_GP predicted slide shape: ',predicted2_slide_STN_GP[\"Class\"].value_counts())\n",
    "    \n",
    "    print(\"---------------STEP4: Data extraction & export -------------------\")\n",
    "# 1) Combining predicted cells & excluded cells (prior to prediction)\n",
    "    \n",
    "    #get non-tau portion\n",
    "    non_tau_portion = predicted1_slide[predicted1_slide[\"Class\"] != \"Tau\"]\n",
    "    \n",
    "    #put everything together: nontau, tau in STR, GP & STN\n",
    "    total_pred = pd.concat([non_tau_portion,\n",
    "                            predicted2_slide_STR,\n",
    "                            predicted2_slide_STN_GP])\n",
    "    \n",
    "    print(\"No loss of cells? \", predicted1_slide.shape[0] == total_pred.shape[0])\n",
    "\n",
    "    output_visualise = total_pred[[\"Image\",\n",
    "                                   \"Name\",\n",
    "                                   \"Parent\",\n",
    "                                   \"Class\",\n",
    "                                   \"Centroid_X\",\n",
    "                                   \"Centroid_Y\",\n",
    "                                   \"Area µm^2\"]]\n",
    "    path_ = (prediction_path +\n",
    "             output_visualise.iloc[0, 0] +\n",
    "             \"_predictions.txt\")\n",
    "    output_visualise.to_csv(path_, sep=\"\\t\", index=False)\n",
    "\n",
    "    print(\"Exported prediction of : \", dat_file)\n",
    "    print(\"---------------------------------------------------\")\n",
    "print(\"Well done, no error!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "f49f039f1826f29992caaab1300810c8c9e5d31d3955aed133543fc6668591e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
