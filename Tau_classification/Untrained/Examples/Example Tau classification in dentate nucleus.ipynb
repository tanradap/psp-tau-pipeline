{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example script: training tau classifier for dentate nucleus regions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in relevant files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\n",
    "                '/Users/mokur/OneDrive - University of Cambridge/Attachments/Jan2023/Tau_pipeline/Tau_classification/')\n",
    "\n",
    "from base import *\n",
    "from constants import *\n",
    "from tau_classification import * "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Screening classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tau        10185\n",
      "Non_tau     9963\n",
      "Name: Class, dtype: int64\n",
      "(20148, 54)\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/mokur/OneDrive - University of Cambridge/Attachments/Jan2023/Tau_pipeline/Tau_classification/Training_data/screening_classifier/\"\n",
    "filename = \"training.txt\"\n",
    "\n",
    "# Create tau database object \n",
    "s_data = TauDataBase(path = path,\n",
    "                     filename = filename) \n",
    "\n",
    "# Prepping data to train screening classifier\n",
    "s_data.classifier1_prep()\n",
    "\n",
    "# Check data\n",
    "print(s_data.c1_data['Class'].value_counts())\n",
    "print(s_data.c1_X_train.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tau classifier for dentate nucleus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Others    1805\n",
      "NFT        234\n",
      "CB         147\n",
      "Name: Class, dtype: int64\n",
      "(2186, 54)\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/mokur/OneDrive - University of Cambridge/Attachments/Jan2023/Tau_pipeline/Tau_classification/Untrained/Training_data/DN/\"\n",
    "filename = \"training.txt\"\n",
    "\n",
    "# Create tau database object \n",
    "dn_data = TauDataBase(path = path,\n",
    "                      filename = filename) \n",
    "\n",
    "# Prepping data to train tau classifier for dn regions\n",
    "dn_data.classifier2_prep()\n",
    "\n",
    "# Check data\n",
    "print(dn_data.c2_data['Class'].value_counts())\n",
    "print(dn_data.c2_X_train.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising & training the classifiers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Screening classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('normalizer', MinMaxScaler()),\n",
       "                ('selector',\n",
       "                 RFE(estimator=RandomForestClassifier(random_state=42),\n",
       "                     n_features_to_select=44)),\n",
       "                ('clf',\n",
       "                 BalancedRandomForestClassifier(class_weight='balanced',\n",
       "                                                max_features=1,\n",
       "                                                max_samples=0.75,\n",
       "                                                min_samples_leaf=2,\n",
       "                                                n_estimators=300,\n",
       "                                                random_state=42,\n",
       "                                                sampling_strategy='not '\n",
       "                                                                  'majority'))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screening_model = ScreeningClassifier(hyperparameters=screening_classifier_hyperparams)\n",
    "screening_model.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "screening_model.train(X=s_data.c1_X_train,\n",
    "                      Y=s_data.c1_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ROI: 0.25 µm per pixel: DAB: Max</td>\n",
       "      <td>0.053625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ROI: 0.25 µm per pixel: DAB: Haralick Sum aver...</td>\n",
       "      <td>0.048330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ROI: 0.25 µm per pixel: Green: Mean</td>\n",
       "      <td>0.041939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ROI: 0.25 µm per pixel: DAB: Mean</td>\n",
       "      <td>0.041595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ROI: 0.25 µm per pixel: Red: Mean</td>\n",
       "      <td>0.040228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             features  importance\n",
       "20                   ROI: 0.25 µm per pixel: DAB: Max    0.053625\n",
       "16  ROI: 0.25 µm per pixel: DAB: Haralick Sum aver...    0.048330\n",
       "26                ROI: 0.25 µm per pixel: Green: Mean    0.041939\n",
       "21                  ROI: 0.25 µm per pixel: DAB: Mean    0.041595\n",
       "35                  ROI: 0.25 µm per pixel: Red: Mean    0.040228"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screening_model.f_importance.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tau classifier for dentate nucleus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('normalizer', MinMaxScaler()),\n",
       "                ('selector',\n",
       "                 RFE(estimator=RandomForestClassifier(random_state=42),\n",
       "                     n_features_to_select=34)),\n",
       "                ('clf',\n",
       "                 BalancedRandomForestClassifier(class_weight='balanced',\n",
       "                                                max_features=0.2,\n",
       "                                                random_state=42,\n",
       "                                                sampling_strategy='not '\n",
       "                                                                  'majority'))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dn_model = TauClassifierNoTA(hyperparameters=dn_classifier_hyperparams)\n",
    "dn_model.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "dn_model.train(X=dn_data.c2_X_train,\n",
    "               Y=dn_data.c2_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (0.409, 0.9144010858386309, 0.9031204850361197, 0.9314285714285713),\n",
       " 1: (0.422, 0.9811932262266609, 0.9714598662207358, 0.9916666666666668),\n",
       " 2: (0.6599999999999999,\n",
       "  0.9944717736676886,\n",
       "  0.9934360958126922,\n",
       "  0.9955770411295273)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dn_model.best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Area µm^2</td>\n",
       "      <td>0.216869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Min diameter µm</td>\n",
       "      <td>0.207962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Max diameter µm</td>\n",
       "      <td>0.058139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Length µm</td>\n",
       "      <td>0.056409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ROI: 0.25 µm per pixel: DAB: Haralick Sum entr...</td>\n",
       "      <td>0.040231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             features  importance\n",
       "0                                           Area µm^2    0.216869\n",
       "4                                     Min diameter µm    0.207962\n",
       "3                                     Max diameter µm    0.058139\n",
       "2                                           Length µm    0.056409\n",
       "19  ROI: 0.25 µm per pixel: DAB: Haralick Sum entr...    0.040231"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dn_model.f_importance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dn_classifier.sav']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(dn_model, 'dn_classifier.sav')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together: tau classification pipeline for novel *dentate nucleus* slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_path = '/Users/mokur/OneDrive - University of Cambridge/Attachments/Jan2023/Detections/DN/'\n",
    "novel_filename = 'detections.txt'\n",
    "prediction_path = \"/Users/mokur/OneDrive - University of Cambridge/Attachments/Jan2023/Predictions_new/DN/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading in files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(novel_path + novel_filename) as f:\n",
    "    mylist = f.read().splitlines()\n",
    "\n",
    "print(\"Read in: \", len(mylist), \"files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tau classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE 721795.svs Detections.txt Number:  1 / 1\n",
      "---------------STEP1: Read in data file -------------------\n",
      "Read in data file: 721795.svs Detections.txt\n",
      "Data shape is: (6006, 61)\n",
      "---------------STEP2: Separating Non-tau from Tau -------------------\n",
      "Tau        3911\n",
      "Non_tau    2095\n",
      "Name: Class, dtype: int64\n",
      "---------------STEP3: Tau hallmark classification -------------------\n",
      "Others       3554\n",
      "CB            223\n",
      "NFT            85\n",
      "Ambiguous      49\n",
      "Name: Class, dtype: int64\n",
      "---------------STEP4: Data extraction & export -------------------\n",
      "No loss of cells?  True\n",
      "Exported prediction of :  721795.svs Detections.txt\n",
      "---------------------------------------------------\n",
      "Well done, no error!\n"
     ]
    }
   ],
   "source": [
    "n_total = len(mylist)\n",
    "faulty_file = []\n",
    "for i in range(0, n_total):\n",
    "\n",
    "    # Read in novel slide\n",
    "    print(\"FILE\", mylist[i], \"Number: \", i + 1, \"/\", n_total)\n",
    "    print(\"---------------STEP1: Read in data file -------------------\")\n",
    "    dat_file = mylist[i]\n",
    "\n",
    "    dat_ = pd.read_csv(novel_path + dat_file, sep=\"\\t\")\n",
    "\n",
    "    # Fixing order of the columns\n",
    "    ordered = dat_[extracted_features]\n",
    "\n",
    "    # Changing column names\n",
    "    # since these names tend to be inconsistent causing problems\n",
    "    ordered.columns.values[5] = \"Centroid_X\"\n",
    "    ordered.columns.values[6] = \"Centroid_Y\"\n",
    "\n",
    "    dat = ordered[ordered[\"Class\"] == \"Unlabelled\"]  # only unlabelled cells\n",
    "    print(\"Read in data file:\", dat_file)\n",
    "    print(\"Data shape is:\", dat.shape)\n",
    "\n",
    "    # Classifier 1: separating Non-tau from Tau\n",
    "    print(\n",
    "        \"---------------STEP2: Separating Non-tau from Tau -------------------\"\n",
    "    )\n",
    "    # 1) Remove NA cells\n",
    "    dat = dat.dropna()\n",
    "\n",
    "    predicted1_slide = dat.copy()\n",
    "\n",
    "    # 2) Dropping extra info features\n",
    "    X_unlabelled = dat.drop(\n",
    "        columns=[\"Image\",\n",
    "                 \"Name\",\n",
    "                 \"Class\",\n",
    "                 \"Parent\",\n",
    "                 \"ROI\",\n",
    "                 \"Centroid_X\",\n",
    "                 \"Centroid_Y\"]\n",
    "    )\n",
    "    # 3) Predictions\n",
    "    screening_model.predict(X_unlabelled)\n",
    "    predicted1_slide[\"Class\"] = screening_model.prediction\n",
    "    print(predicted1_slide[\"Class\"].value_counts())\n",
    "\n",
    "    # Classifier 2: tau hallmark classification\n",
    "    print(\n",
    "        \"---------------STEP3: Tau hallmark classification -------------------\"\n",
    "    )\n",
    "    # Select out 'tau' portion only (ignoring non-tau & ambiguous cells )\n",
    "    tau_portion = predicted1_slide[predicted1_slide[\"Class\"] == \"Tau\"]\n",
    "    if tau_portion.shape[0] == 0:\n",
    "        print(\"There is no tau on this slide!\")\n",
    "        faulty_file.append(dat[\"Image\"][0] + \" No tau on the slide\")\n",
    "        continue\n",
    "    tau_portion_X = tau_portion.drop(\n",
    "        columns=[\"Image\",\n",
    "                 \"Name\",\n",
    "                 \"Class\",\n",
    "                 \"Parent\",\n",
    "                 \"ROI\",\n",
    "                 \"Centroid_X\",\n",
    "                 \"Centroid_Y\"])\n",
    "    predicted2_slide = tau_portion.copy()\n",
    "    # Separates out detections that are not tau\n",
    "    non_tau_portion = predicted1_slide[predicted1_slide[\"Class\"] != \"Tau\"]\n",
    "\n",
    "    # Make predictions on Tau objects\n",
    "    dn_model.predict(tau_portion_X)\n",
    "    predicted2_slide[\"Class\"] = dn_model.prediction\n",
    "    print(predicted2_slide[\"Class\"].value_counts())\n",
    "\n",
    "    # Extracting data out\n",
    "    print(\"---------------STEP4: Data extraction & export -------------------\")\n",
    "\n",
    "    # 1) Combining predicted cells & excluded cells (prior to prediction)\n",
    "    total_pred = pd.concat([non_tau_portion,\n",
    "                            predicted2_slide])\n",
    "    print(\"No loss of cells? \",\n",
    "          predicted1_slide.shape[0] == total_pred.shape[0])\n",
    "\n",
    "    output_visualise = total_pred[[\"Image\",\n",
    "                                   \"Class\",\n",
    "                                   \"Centroid_X\",\n",
    "                                   \"Centroid_Y\",\n",
    "                                   \"Area µm^2\"]]\n",
    "    path_ = (\n",
    "            prediction_path +\n",
    "            output_visualise.iloc[0, 0] +\n",
    "            \"_predictions.txt\"\n",
    "            )\n",
    "    output_visualise.to_csv(path_, sep=\"\\t\", index=False)\n",
    "\n",
    "    print(\"Exported prediction of : \", dat_file)\n",
    "    print(\"---------------------------------------------------\")\n",
    "print(\"Well done, no error!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "f49f039f1826f29992caaab1300810c8c9e5d31d3955aed133543fc6668591e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
